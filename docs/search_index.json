[["index.html", "Exploratory Data Analysis Essentials Section1 Summary", " Exploratory Data Analysis Essentials Rena Marrotta 2024-02-04 Section1 Summary The United States National Highway Traffic Safety Administration provides a dataset of over 27,000 recalls recorded in the US from 1966 to 2024. I will be using this data to illustrate EDA Essentials as a data scientist. Source: https://datahub.transportation.gov/Automobiles/Recalls-Data/6axg-epim/about_data "],["data-overview.html", "Section2 Data Overview", " Section2 Data Overview The first step of the EDA process, is to get to know your data by understanding what exactly it contains. I will walk through the following EDA steps in both R and Python: Data shape Column names Data types Missing values library(reticulate) import pandas as pd #load in data df = pd.read_csv(&#39;Recalls_Data_20240204.csv&#39;) We have a total of 15 variables and 27,648 observations. #number of observations and variables df.shape ## (27648, 15) By printing all column names, we can see that “Park Outside Advisory” contains an extra space at the end which should be removed for consistency. #list of column names cols = df.columns cols ## Index([&#39;Report Received Date&#39;, &#39;NHTSA ID&#39;, &#39;Recall Link&#39;, &#39;Manufacturer&#39;, ## &#39;Subject&#39;, &#39;Component&#39;, &#39;Mfr Campaign Number&#39;, &#39;Recall Type&#39;, ## &#39;Potentially Affected&#39;, &#39;Recall Description&#39;, &#39;Consequence Summary&#39;, ## &#39;Corrective Action&#39;, &#39;Park Outside Advisory &#39;, &#39;Do Not Drive Advisory&#39;, ## &#39;Completion Rate % (Blank - Not Reported)&#39;], ## dtype=&#39;object&#39;) By taking a subset of 20 rows of the data we can easily view the data to better understand the information it contains.Some of the object variables provide lengthy descriptions, including “Subject”, Recall Description”, “Consequence Summary”, and “Corrective Action” while “Recall Link” contains a https link to the recall information. head = df.head(20) head ## Report Received Date ... Completion Rate % (Blank - Not Reported) ## 0 01/31/2024 ... NaN ## 1 01/31/2024 ... NaN ## 2 01/30/2024 ... NaN ## 3 01/30/2024 ... NaN ## 4 01/30/2024 ... NaN ## 5 01/30/2024 ... NaN ## 6 01/26/2024 ... NaN ## 7 01/26/2024 ... NaN ## 8 01/26/2024 ... NaN ## 9 01/26/2024 ... NaN ## 10 01/26/2024 ... NaN ## 11 01/26/2024 ... NaN ## 12 01/25/2024 ... NaN ## 13 01/25/2024 ... NaN ## 14 01/25/2024 ... NaN ## 15 01/25/2024 ... NaN ## 16 01/25/2024 ... NaN ## 17 01/24/2024 ... NaN ## 18 01/23/2024 ... NaN ## 19 01/23/2024 ... NaN ## ## [20 rows x 15 columns] We can also see that we only have 2 numeric variables. The “Report Received Date” variable needs to be transformed to a datetime object and “Park Outside Advisory” and “Do Not Drive Advisory” both need to be transformed to Boolean objects. df.dtypes ## Report Received Date object ## NHTSA ID object ## Recall Link object ## Manufacturer object ## Subject object ## Component object ## Mfr Campaign Number object ## Recall Type object ## Potentially Affected float64 ## Recall Description object ## Consequence Summary object ## Corrective Action object ## Park Outside Advisory object ## Do Not Drive Advisory object ## Completion Rate % (Blank - Not Reported) float64 ## dtype: object I will rename and redefine variable types in the following section. #rename df = df.rename(columns = {&#39;Park Outside Advisory &#39;: &#39;Park Outside Advisory&#39;}) # change data types df[&#39;Park Outside Advisory&#39;].unique() #No and Yes are the only unique values ## array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) df[&#39;Park Outside Advisory&#39;] = df[&#39;Park Outside Advisory&#39;].map({&#39;Yes&#39;: True, &#39;No&#39;: False}) df[&#39;Do Not Drive Advisory&#39;].unique() #No and Yes are the only unique values ## array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) df[&#39;Do Not Drive Advisory&#39;] = df[&#39;Do Not Drive Advisory&#39;].map({&#39;Yes&#39;: True, &#39;No&#39;: False}) from datetime import datetime df[&#39;Report Received Date&#39;] = pd.to_datetime(df[&#39;Report Received Date&#39;]) Now our data types are representative of what they should be. df.dtypes ## Report Received Date datetime64[ns] ## NHTSA ID object ## Recall Link object ## Manufacturer object ## Subject object ## Component object ## Mfr Campaign Number object ## Recall Type object ## Potentially Affected float64 ## Recall Description object ## Consequence Summary object ## Corrective Action object ## Park Outside Advisory bool ## Do Not Drive Advisory bool ## Completion Rate % (Blank - Not Reported) float64 ## dtype: object Now that we have our data types correctly defined, we can continue our EDA. df[df.duplicated()] #checks duplicate values across all columns ## Empty DataFrame ## Columns: [Report Received Date, NHTSA ID, Recall Link, Manufacturer, Subject, Component, Mfr Campaign Number, Recall Type, Potentially Affected, Recall Description, Consequence Summary, Corrective Action, Park Outside Advisory, Do Not Drive Advisory, Completion Rate % (Blank - Not Reported)] ## Index: [] df[df[&#39;NHTSA ID&#39;].duplicated()] #checks for duplicated unique ID&#39;s ## Empty DataFrame ## Columns: [Report Received Date, NHTSA ID, Recall Link, Manufacturer, Subject, Component, Mfr Campaign Number, Recall Type, Potentially Affected, Recall Description, Consequence Summary, Corrective Action, Park Outside Advisory, Do Not Drive Advisory, Completion Rate % (Blank - Not Reported)] ## Index: [] The following code is used to identify the percentage of missing values by column in the dataframe: # calculates the sum of true values for each column and calculates the % of each column missing and normalizes it by the total number of rows percent_missing = df.isnull().sum() * 100 / len(df) # create a missing value data frame and sort in descending missing_vals = pd.DataFrame({&#39;colum_name&#39;: df.columns, &#39;percent_missing&#39;: percent_missing}).reset_index(drop = True) missing_vals.sort_values(by = &#39;percent_missing&#39;, ascending = False) ## colum_name percent_missing ## 14 Completion Rate % (Blank - Not Reported) 66.919850 ## 10 Consequence Summary 17.679398 ## 9 Recall Description 8.684172 ## 11 Corrective Action 8.637153 ## 6 Mfr Campaign Number 0.162760 ## 8 Potentially Affected 0.148293 ## 0 Report Received Date 0.000000 ## 1 NHTSA ID 0.000000 ## 2 Recall Link 0.000000 ## 3 Manufacturer 0.000000 ## 4 Subject 0.000000 ## 5 Component 0.000000 ## 7 Recall Type 0.000000 ## 12 Park Outside Advisory 0.000000 ## 13 Do Not Drive Advisory 0.000000 Now we can see that Completion Rate % and Consequence Summary have the highest percentage of missing values. For Completion Rate % we can see that NA indicates that the percentage of remedied units out of the total recall population was not reported. Missingness for Consequence Summary, Recall Description, and Corrective Action, and Mfr Campaign Number can all be imputed as missing since they are categorical descriptions. Potentially Affected could possibly be imputed by the mean or median, but further analysis will be required to determine the best imputation for that variable. NOTE When predicting a target variable, imputation should wait until the dataset is split into train, validation, and test because if we will want to impute training data information. For this EDA Essentials Tutorial, I will not be predicting a target variable, so this is not a concern. #looking for any obvious patterns miss_check = df[df[&#39;Potentially Affected&#39;].isnull() == True] # found that NR (Not Reported) is a value in Mfr Campaign Number # are there other rows that indicate missingness through &#39;Not Reported&#39;? rows_with_not_reported = df[df.applymap(lambda x: &#39;Not Reported&#39; in str(x)).any(axis=1)] # looks to be only in Mfr Campaign Number The following code imputes categorical missing values. I decided to follow the missing convention for Mfr Campaign Number and Completion Rate % because these columns indicate a ‘Not Reporting’ option. For the other categorical variables, I imputed ‘M’ to indicate a missing value. import numpy as np cat_missing = [&#39;Consequence Summary&#39;, &#39;Recall Description&#39;, &#39;Corrective Action&#39;] for var in cat_missing: df[var] = np.where(df[var].isnull(), &#39;M&#39;, df[var]) df[&#39;Mfr Campaign Number&#39;] = np.where(df[&#39;Mfr Campaign Number&#39;].isnull(), &#39;NR (Not Reported)&#39;, df[&#39;Mfr Campaign Number&#39;]) df[&#39;Completion Rate % (Blank - Not Reported)&#39;] = np.where(df[&#39;Completion Rate % (Blank - Not Reported)&#39;].isnull(), &#39;(Not Reported)&#39;, df[&#39;Completion Rate % (Blank - Not Reported)&#39;]) df.isnull().sum() * 100 / len(df) ## Report Received Date 0.000000 ## NHTSA ID 0.000000 ## Recall Link 0.000000 ## Manufacturer 0.000000 ## Subject 0.000000 ## Component 0.000000 ## Mfr Campaign Number 0.000000 ## Recall Type 0.000000 ## Potentially Affected 0.148293 ## Recall Description 0.000000 ## Consequence Summary 0.000000 ## Corrective Action 0.000000 ## Park Outside Advisory 0.000000 ## Do Not Drive Advisory 0.000000 ## Completion Rate % (Blank - Not Reported) 0.000000 ## dtype: float64 "],["variable-assessment-and-visualization.html", "Section3 Variable Assessment and Visualization", " Section3 Variable Assessment and Visualization "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
