[["index.html", "Exploratory Data Analysis Essentials Section1 Summary", " Exploratory Data Analysis Essentials Rena Marrotta 2024-02-06 Section1 Summary The United States National Highway Traffic Safety Administration provides a dataset of over 27,000 recalls recorded in the US from 1966 to 2024. I will be using this data to illustrate EDA Essentials as a data scientist. Source: https://datahub.transportation.gov/Automobiles/Recalls-Data/6axg-epim/about_data "],["data-overview.html", "Section2 Data Overview", " Section2 Data Overview The first step of the EDA process, is to get to know your data by understanding what exactly it contains. I will walk through the following EDA steps in Python. Data shape Column names Data types Missing values library(reticulate) import pandas as pd #load in data df = pd.read_csv(&#39;Recalls_Data_20240204.csv&#39;) We have a total of 15 variables and 27,648 observations. #number of observations and variables df.shape ## (27648, 15) By printing all column names, we can see that “Park Outside Advisory” contains an extra space at the end which should be removed for consistency. #list of column names cols = df.columns cols ## Index([&#39;Report Received Date&#39;, &#39;NHTSA ID&#39;, &#39;Recall Link&#39;, &#39;Manufacturer&#39;, ## &#39;Subject&#39;, &#39;Component&#39;, &#39;Mfr Campaign Number&#39;, &#39;Recall Type&#39;, ## &#39;Potentially Affected&#39;, &#39;Recall Description&#39;, &#39;Consequence Summary&#39;, ## &#39;Corrective Action&#39;, &#39;Park Outside Advisory &#39;, &#39;Do Not Drive Advisory&#39;, ## &#39;Completion Rate % (Blank - Not Reported)&#39;], ## dtype=&#39;object&#39;) By taking a subset of 20 rows of the data we can easily view the data to better understand the information it contains.Some of the object variables provide lengthy descriptions, including “Subject”, Recall Description”, “Consequence Summary”, and “Corrective Action” while “Recall Link” contains a https link to the recall information. head = df.head(20) head ## Report Received Date ... Completion Rate % (Blank - Not Reported) ## 0 01/31/2024 ... NaN ## 1 01/31/2024 ... NaN ## 2 01/30/2024 ... NaN ## 3 01/30/2024 ... NaN ## 4 01/30/2024 ... NaN ## 5 01/30/2024 ... NaN ## 6 01/26/2024 ... NaN ## 7 01/26/2024 ... NaN ## 8 01/26/2024 ... NaN ## 9 01/26/2024 ... NaN ## 10 01/26/2024 ... NaN ## 11 01/26/2024 ... NaN ## 12 01/25/2024 ... NaN ## 13 01/25/2024 ... NaN ## 14 01/25/2024 ... NaN ## 15 01/25/2024 ... NaN ## 16 01/25/2024 ... NaN ## 17 01/24/2024 ... NaN ## 18 01/23/2024 ... NaN ## 19 01/23/2024 ... NaN ## ## [20 rows x 15 columns] We can also see that we only have 2 numeric variables. The “Report Received Date” variable needs to be transformed to a datetime object and “Park Outside Advisory” and “Do Not Drive Advisory” both need to be transformed to Boolean objects. df.dtypes ## Report Received Date object ## NHTSA ID object ## Recall Link object ## Manufacturer object ## Subject object ## Component object ## Mfr Campaign Number object ## Recall Type object ## Potentially Affected float64 ## Recall Description object ## Consequence Summary object ## Corrective Action object ## Park Outside Advisory object ## Do Not Drive Advisory object ## Completion Rate % (Blank - Not Reported) float64 ## dtype: object I will rename and redefine variable types in the following section. #rename df = df.rename(columns = {&#39;Park Outside Advisory &#39;: &#39;Park Outside Advisory&#39;}) # change data types df[&#39;Park Outside Advisory&#39;].unique() #No and Yes are the only unique values ## array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) df[&#39;Park Outside Advisory&#39;] = df[&#39;Park Outside Advisory&#39;].map({&#39;Yes&#39;: True, &#39;No&#39;: False}) df[&#39;Do Not Drive Advisory&#39;].unique() #No and Yes are the only unique values ## array([&#39;No&#39;, &#39;Yes&#39;], dtype=object) df[&#39;Do Not Drive Advisory&#39;] = df[&#39;Do Not Drive Advisory&#39;].map({&#39;Yes&#39;: True, &#39;No&#39;: False}) from datetime import datetime df[&#39;Report Received Date&#39;] = pd.to_datetime(df[&#39;Report Received Date&#39;]) Now our data types are representative of what they should be. df.dtypes ## Report Received Date datetime64[ns] ## NHTSA ID object ## Recall Link object ## Manufacturer object ## Subject object ## Component object ## Mfr Campaign Number object ## Recall Type object ## Potentially Affected float64 ## Recall Description object ## Consequence Summary object ## Corrective Action object ## Park Outside Advisory bool ## Do Not Drive Advisory bool ## Completion Rate % (Blank - Not Reported) float64 ## dtype: object Now that we have our data types correctly defined, we can continue our EDA. df[df.duplicated()] #checks duplicate values across all columns ## Empty DataFrame ## Columns: [Report Received Date, NHTSA ID, Recall Link, Manufacturer, Subject, Component, Mfr Campaign Number, Recall Type, Potentially Affected, Recall Description, Consequence Summary, Corrective Action, Park Outside Advisory, Do Not Drive Advisory, Completion Rate % (Blank - Not Reported)] ## Index: [] df[df[&#39;NHTSA ID&#39;].duplicated()] #checks for duplicated unique ID&#39;s ## Empty DataFrame ## Columns: [Report Received Date, NHTSA ID, Recall Link, Manufacturer, Subject, Component, Mfr Campaign Number, Recall Type, Potentially Affected, Recall Description, Consequence Summary, Corrective Action, Park Outside Advisory, Do Not Drive Advisory, Completion Rate % (Blank - Not Reported)] ## Index: [] The following code is used to identify the percentage of missing values by column in the dataframe: # calculates the sum of true values for each column and calculates the % of each column missing and normalizes it by the total number of rows percent_missing = df.isnull().sum() * 100 / len(df) # create a missing value data frame and sort in descending missing_vals = pd.DataFrame({&#39;colum_name&#39;: df.columns, &#39;percent_missing&#39;: percent_missing}).reset_index(drop = True) missing_vals.sort_values(by = &#39;percent_missing&#39;, ascending = False) ## colum_name percent_missing ## 14 Completion Rate % (Blank - Not Reported) 66.919850 ## 10 Consequence Summary 17.679398 ## 9 Recall Description 8.684172 ## 11 Corrective Action 8.637153 ## 6 Mfr Campaign Number 0.162760 ## 8 Potentially Affected 0.148293 ## 0 Report Received Date 0.000000 ## 1 NHTSA ID 0.000000 ## 2 Recall Link 0.000000 ## 3 Manufacturer 0.000000 ## 4 Subject 0.000000 ## 5 Component 0.000000 ## 7 Recall Type 0.000000 ## 12 Park Outside Advisory 0.000000 ## 13 Do Not Drive Advisory 0.000000 Now we can see that Completion Rate % and Consequence Summary have the highest percentage of missing values. For Completion Rate % we can see that NA indicates that the percentage of remedied units out of the total recall population was not reported. Missingness for Consequence Summary, Recall Description, and Corrective Action, and Mfr Campaign Number can all be imputed as missing since they are categorical descriptions. Potentially Affected could possibly be imputed by the mean or median, but further analysis will be required to determine the best imputation for that variable. NOTE When predicting a target variable, imputation should wait until the dataset is split into train, validation, and test because if we will want to impute training data information. For this EDA Essentials Tutorial, I will not be predicting a target variable, so this is not a concern. #looking for any obvious patterns miss_check = df[df[&#39;Potentially Affected&#39;].isnull() == True] # found that NR (Not Reported) is a value in Mfr Campaign Number # are there other rows that indicate missingness through &#39;Not Reported&#39;? rows_with_not_reported = df[df.applymap(lambda x: &#39;Not Reported&#39; in str(x)).any(axis=1)] # looks to be only in Mfr Campaign Number The following code imputes categorical missing values. I decided to follow the missing convention for Mfr Campaign Number and Completion Rate % because these columns indicate a ‘Not Reporting’ option. For the other categorical variables, I imputed ‘M’ to indicate a missing value. import numpy as np cat_missing = [&#39;Consequence Summary&#39;, &#39;Recall Description&#39;, &#39;Corrective Action&#39;] for var in cat_missing: df[var] = np.where(df[var].isnull(), &#39;M&#39;, df[var]) df[&#39;Mfr Campaign Number&#39;] = np.where(df[&#39;Mfr Campaign Number&#39;].isnull(), &#39;NR (Not Reported)&#39;, df[&#39;Mfr Campaign Number&#39;]) df[&#39;Completion Rate % (Blank - Not Reported)&#39;] = np.where(df[&#39;Completion Rate % (Blank - Not Reported)&#39;].isnull(), &#39;(Not Reported)&#39;, df[&#39;Completion Rate % (Blank - Not Reported)&#39;]) df.isnull().sum() * 100 / len(df) ## Report Received Date 0.000000 ## NHTSA ID 0.000000 ## Recall Link 0.000000 ## Manufacturer 0.000000 ## Subject 0.000000 ## Component 0.000000 ## Mfr Campaign Number 0.000000 ## Recall Type 0.000000 ## Potentially Affected 0.148293 ## Recall Description 0.000000 ## Consequence Summary 0.000000 ## Corrective Action 0.000000 ## Park Outside Advisory 0.000000 ## Do Not Drive Advisory 0.000000 ## Completion Rate % (Blank - Not Reported) 0.000000 ## dtype: float64 What’s also great about having a datetime object is you can easily create year, quarter, and month columns based on the date. df[&#39;year&#39;] = df[&#39;Report Received Date&#39;].dt.year df[&#39;quarter&#39;] = df[&#39;Report Received Date&#39;].dt.quarter df[&#39;month&#39;] = df[&#39;Report Received Date&#39;].dt.month "],["variable-assessment-and-visualization.html", "Section3 Variable Assessment and Visualization", " Section3 Variable Assessment and Visualization This data is mostly categorical information, so we can visualize the data using bar graphs. import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from collections import defaultdict The following code below builds a bar chart that visualizes the value counts of each category in the variable ‘Recall Type’. Normalize returns the relative frequency by dividing all values by the sum of values. I am also using a seaborn color palette to define the colors of the bars. I rotate the x-axis labels and tighten the layout to ensure that no text is cut off. Plots can be as simple or as complicated as you choose. I chose to make simple visualizations since their purpose is to understand the data but not to for a report. This plot illustrates that the majority of recall type’s are in the vehicle category. sns.set_palette(&quot;pastel&quot;) #seaborn color palette set df[&#39;Recall Type&#39;].value_counts(normalize=True).plot(kind=&#39;bar&#39;, title=&quot;Recall Type&quot;, figsize =(5,4), color = sns.color_palette()) plt.xlabel(&#39;Recall Type&#39;) plt.xticks(rotation=0, ha = &#39;center&#39;) #horizontal x-axis tick labels, centered ## (array([0, 1, 2, 3]), [Text(0, 0, &#39;Vehicle&#39;), Text(1, 0, &#39;Equipment&#39;), Text(2, 0, &#39;Tire&#39;), Text(3, 0, &#39;Child Seat&#39;)]) plt.tight_layout() plt.show() Nearly 100% of the recalls in the entire dataset did not require a do not drive advisory. plt.clf() sns.set_palette(&quot;dark&quot;) df[&#39;Do Not Drive Advisory&#39;].value_counts(normalize=True).plot(kind=&#39;bar&#39;, title=&quot;Do Not Drive Advisory&quot;, figsize =(5,4), color = sns.color_palette()) plt.xlabel(&#39;Do Not Drive Advisory&#39;) plt.xticks(rotation=0, ha = &#39;center&#39;) ## (array([0, 1]), [Text(0, 0, &#39;False&#39;), Text(1, 0, &#39;True&#39;)]) plt.tight_layout() plt.show() The following plot summarizes the top 5 largest manufacturers based on the proportion of each manufacturer present in the data set. We can see that General Motors take up the largest proportion of the data, around 6%. We also can see that Ford and Chrysler take up around 5% each and then the proportions drop off. plt.clf() sns.set_palette(&quot;colorblind&quot;) df[&#39;Manufacturer&#39;].value_counts(normalize=True).nlargest(5).plot(kind=&#39;bar&#39;, title=&quot;Manufacturer&quot;, figsize =(5,4), color = sns.color_palette()) plt.xlabel(&#39;Manufacturer&#39;) plt.xticks(rotation=25, ha = &#39;center&#39;, fontsize = 10) ## (array([0, 1, 2, 3, 4]), [Text(0, 0, &#39;General Motors, LLC&#39;), Text(1, 0, &#39;Ford Motor Company&#39;), Text(2, 0, &#39;Chrysler (FCA US, LLC)&#39;), Text(3, 0, &#39;Volkswagen Group of America, Inc.&#39;), Text(4, 0, &#39;Forest River, Inc.&#39;)]) plt.tight_layout() plt.show() When we look at top 10 manufactuers, we can still see that after the drop from Chrysler to Volkswagen it remains relatively consistent for the remaining 7 manufacturers at about 2-1%. plt.clf() df[&#39;Manufacturer&#39;].value_counts(normalize=True).nlargest(10).plot(kind=&#39;bar&#39;, title=&quot;Manufacturer&quot;, figsize =(5,4), color = sns.color_palette()) plt.xlabel(&#39;Manufacturer&#39;) plt.xticks(rotation=20, ha = &#39;center&#39;, fontsize = 7) ## (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), [Text(0, 0, &#39;General Motors, LLC&#39;), Text(1, 0, &#39;Ford Motor Company&#39;), Text(2, 0, &#39;Chrysler (FCA US, LLC)&#39;), Text(3, 0, &#39;Volkswagen Group of America, Inc.&#39;), Text(4, 0, &#39;Forest River, Inc.&#39;), Text(5, 0, &#39;BMW of North America, LLC&#39;), Text(6, 0, &#39;Daimler Trucks North America, LLC&#39;), Text(7, 0, &#39;Mercedes-Benz USA, LLC&#39;), Text(8, 0, &#39;Honda (American Honda Motor Co.)&#39;), Text(9, 0, &#39;Nissan North America, Inc.&#39;)]) plt.tight_layout() plt.show() I created a new data set to only look at vehicle recalls in the last 5 years. vehicle_2019 = df[(df[&#39;Recall Type&#39;] == &#39;Vehicle&#39;) &amp; (df[&#39;year&#39;] &gt;= 2019)] We can look at the same type of information at a more zoomed in perspective by narrowing our data window. We see a similar pattern in more recent years. plt.clf() vehicle_2019[&#39;Manufacturer&#39;].value_counts(normalize=True).nlargest(10).plot(kind=&#39;bar&#39;, title=&quot;Manufacturer&quot;, figsize =(5,4), color = sns.color_palette()) plt.xlabel(&#39;Manufacturer&#39;) plt.xticks(rotation=20, ha = &#39;center&#39;, fontsize = 7) ## (array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9]), [Text(0, 0, &#39;Ford Motor Company&#39;), Text(1, 0, &#39;Forest River, Inc.&#39;), Text(2, 0, &#39;Mercedes-Benz USA, LLC&#39;), Text(3, 0, &#39;Daimler Trucks North America, LLC&#39;), Text(4, 0, &#39;Chrysler (FCA US, LLC)&#39;), Text(5, 0, &#39;Volkswagen Group of America, Inc.&#39;), Text(6, 0, &#39;BMW of North America, LLC&#39;), Text(7, 0, &#39;General Motors, LLC&#39;), Text(8, 0, &#39;Jayco, Inc.&#39;), Text(9, 0, &#39;Hyundai Motor America&#39;)]) plt.tight_layout() plt.show() When we plot the continuous variable ‘Potentially Affected’ for vehicle recalls between 2019-2024 we can see that the the spread of the data is concentrated around the minimum value. plt.clf() sns.boxplot(vehicle_2019[&#39;Potentially Affected&#39;]) plt.title(&#39;Box Plot&#39;) plt.xlabel(&#39;Data&#39;) plt.ylabel(&#39;Values&#39;) plt.grid(True) plt.show() I wanted to perform separate visualizations of outliers that are outliers and those that are not. I filtered any observations that fell above or below a threshold of +/- 1.5 * IQR which is a common way to identify outliers. I then removed those observations form the dataset so I could visualize what the spread of the data looks like without being influenced by outliers. Q1 = np.percentile(vehicle_2019[&#39;Potentially Affected&#39;], 25) Q3 = np.percentile(vehicle_2019[&#39;Potentially Affected&#39;], 75) # Calculate IQR IQR = Q3 - Q1 # Define outlier thresholds lower_threshold = Q1 - 1.5 * IQR upper_threshold = Q3 + 1.5 * IQR # Identify outliers outliers = vehicle_2019[(vehicle_2019[&#39;Potentially Affected&#39;] &lt; lower_threshold) | (vehicle_2019[&#39;Potentially Affected&#39;] &gt; upper_threshold)] outliers_ids = outliers[&#39;NHTSA ID&#39;] vehicle_2019_nouts = vehicle_2019[~vehicle_2019[&#39;NHTSA ID&#39;].isin(outliers_ids)] We can now see that our range goes from 0 to 10,000. The data is still strongly right skewed suggesting that majority of the vehicle recalls only affected a small group of the population. plt.clf() sns.histplot(data=vehicle_2019_nouts, x=&#39;Potentially Affected&#39;, bins=50); plt.title(&quot;Histogram&quot;) plt.show() plt.clf() sns.boxplot(vehicle_2019_nouts[&#39;Potentially Affected&#39;]) plt.title(&#39;Box Plot&#39;) plt.xlabel(&#39;Data&#39;) plt.ylabel(&#39;Values&#39;) plt.grid(True) plt.show() In our outlier data we can see the spread is extremely wide with an observation where 3.5 million people were potentially affected. plt.clf() sns.histplot(data=outliers, x=&#39;Potentially Affected&#39;, bins = 100) plt.ticklabel_format(style=&#39;plain&#39;, axis=&#39;x&#39;) plt.title(&quot;Histogram&quot;) plt.show() It can be difficult to tell where the range of values really begins by taking the min we can see that the first value in the outliers is 10,969 if we wanted to know the exact value. outliers[&#39;Potentially Affected&#39;].min() ## 10969.0 "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
